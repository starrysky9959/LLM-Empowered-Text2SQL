{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sqlite3\n",
    "from constants import DATABASE_PATH_PATTERN, DEV_JSON_PATH, TRAIN_JSON_PATH\n",
    "import util\n",
    "import json\n",
    "import sqlparse\n",
    "from thefuzz import process\n",
    "\n",
    "with open(\n",
    "    \"/home/bingxing2/home/scx8900/projects/bird_train/train/train_tables.json\", \"r\"\n",
    ") as input_file:\n",
    "    table_info = json.load(input_file)\n",
    "comment_map = {}\n",
    "for table in table_info:\n",
    "    curr = {}\n",
    "    table_names_original = table[\"table_names_original\"]\n",
    "    column_names = table[\"column_names\"]\n",
    "    column_names_original = table[\"column_names_original\"]\n",
    "    for i in range(len(column_names_original)):\n",
    "        table_index = column_names_original[i][0]\n",
    "        if table_index<0:\n",
    "            continue\n",
    "        table_name = table_names_original[table_index].lower()\n",
    "        format_column_name = f\"{table_name}.{column_names_original[i][1]}\".lower()\n",
    "        comment = column_names[i][1]\n",
    "        curr[format_column_name] = comment\n",
    "    comment_map[table[\"db_id\"]] = curr\n",
    "\n",
    "with open(\"bird_train_comment.json\", \"w\") as output_file:\n",
    "    json.dump(comment_map,output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../bird_train/train/train.json\", \"r\") as dataset_file:\n",
    "    dataset = json.load(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: no such column: t2.stuid\n",
      "Detailed error message: ('no such column: t2.stuid',)\n",
      "SQLite error code: 1\n",
      "SQLite error name: SQLITE_ERROR\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "import sqlite3\n",
    "from constants import DATABASE_PATH_PATTERN, DEV_JSON_PATH, TRAIN_JSON_PATH\n",
    "import util\n",
    "import json\n",
    "import sqlparse\n",
    "from thefuzz import process\n",
    "\n",
    "db_id = \"pets_1\"\n",
    "db_path = DATABASE_PATH_PATTERN.format(db_id=db_id)\n",
    "sqlite3.enable_callback_tracebacks(True)\n",
    "try:\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # 创建一个游标对象\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # 执行一个SQL语句\n",
    "    cursor.execute('SELECT stuid FROM student EXCEPT SELECT t2.stuid FROM has_pet AS t1 JOIN pets AS t2 ON t1.petid = t2.petid WHERE t2.pettype = \"cat\";')\n",
    "\n",
    "    # 获取查询结果\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # 打印结果\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "\n",
    "\n",
    "except sqlite3.Error as e:\n",
    "    # 打印SQLite错误信息\n",
    "    print(f\"An error occurred: {e.args[0]}\")\n",
    "    # 如果有详细信息，打印详细信息\n",
    "    # if len(e.args) > 1:\n",
    "    print(f\"Detailed error message: {e.args}\")\n",
    "    # 打印SQLite错误码和错误名称\n",
    "    print(f\"SQLite error code: {e.sqlite_errorcode}\")\n",
    "    print(f\"SQLite error name: {e.sqlite_errorname}\")\n",
    "\n",
    "finally:\n",
    "    # 关闭游标和数据库连接\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if conn:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bird_alpaca.json\", \"w\") as f:\n",
    "    json.dump(bird_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(TRAIN_JSON_PATH, \"r\") as dataset_file:\n",
    "    dataset = json.load(dataset_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = []\n",
    "for case in dataset[:]:\n",
    "    db_path = DATABASE_PATH_PATTERN.format(db_id=case[\"db_id\"])\n",
    "    schema = util.get_sqlite_alpaca_sft_prompt_schema(db_path)\n",
    "\n",
    "    schema_str = \"\"\n",
    "    for table, columns in schema.items():\n",
    "        columns_str = \", \".join(columns)\n",
    "        schema_str += f\"{table}[{columns_str}]\"\n",
    "        schema_str += \"\\n\"\n",
    "    instruction = INSTRUCTION_PATTERN.format(\n",
    "        schema=schema_str,\n",
    "        question=case[\"question\"],\n",
    "    )\n",
    "    sql = case[\"query\"]\n",
    "    sql = \" \".join(sql.split())\n",
    "    output = OUTPUT_PATTERN.format(sql=sql)\n",
    "    # print(instruction)\n",
    "    # print(output)\n",
    "    final_dataset.append({\"instruction\": instruction, \"output\": output})\n",
    "with open(\"spider_alpaca.json\", \"w\") as f:\n",
    "    json.dump(final_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8659\n",
      "9428\n",
      "17613\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"spider_alpaca.json\", \"r\") as dataset_file:\n",
    "    spider = json.load(dataset_file)\n",
    "with open(\"bird_alpaca.json\", \"r\") as dataset_file:\n",
    "    bird = json.load(dataset_file)\n",
    "print(len(spider))\n",
    "print(len(bird))\n",
    "\n",
    "all_set = []\n",
    "for case in spider:\n",
    "    if len(case[\"instruction\"]) < 10000:\n",
    "        case[\"system\"] = \"You are a database expert.\\n\"\n",
    "        all_set.append(case)\n",
    "for case in bird:\n",
    "    if len(case[\"instruction\"]) < 10000:\n",
    "        case[\"system\"] = \"You are a database expert.\\n\"\n",
    "        all_set.append(case)\n",
    "print(len(all_set))\n",
    "with open(\"text2sql.json\", \"w\") as f:\n",
    "    json.dump(all_set, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "15535\n"
     ]
    }
   ],
   "source": [
    "all_set = spider+bird_trunc\n",
    "max_len=0\n",
    "all_trunc = []\n",
    "for case in all_set:\n",
    "    if len(case[\"instruction\"]) < 4000:\n",
    "        all_trunc.append(case)\n",
    "# print(all_trunc)\n",
    "print(type(all_trunc))\n",
    "print(len(all_trunc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"spider_bird.json\", \"w\") as f:\n",
    "    json.dump(all_trunc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from constants import DATABASE_PATH_PATTERN, DEV_JSON_PATH\n",
    "import util\n",
    "\n",
    "print(torch.version.cuda)\n",
    "model_path = \"../deepseek-coder-6.7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4551015\n"
     ]
    }
   ],
   "source": [
    "from prompt import SYSTEM_PROMPT\n",
    "\n",
    "\n",
    "with open(\"cot_dataset/train/step_2.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "total = 0 \n",
    "for case in dataset:\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": case[\"instruction\"],\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": case[\"output\"]},\n",
    "    ]\n",
    "    tokens = len(tokenizer.apply_chat_template(conversation, tokenize=True))\n",
    "    total += tokens\n",
    "print(total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
