{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Table: mobile_data\\n  Column: 日期, Type: DATE\\n  Column: 网关SN, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: 宽带账号, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: 网关mac, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: 中断总次数, Type: INTEGER\\n  Column: 中断总时长(s), Type: INTEGER\\n  Column: 厂商, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: 型号, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: 地市, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: 区县, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: 网格, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: 小区名称, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: 小区ID, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: 覆盖方式, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: 分纤箱, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: 鉴权码, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: PON口名称, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: 宽带类型, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: 二级分光器名称, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: OLT名称, Type: VARCHAR(255) COLLATE \"utf8mb4_unicode_ci\"\\n  Column: 时间段, Type: INTEGER\\n  Primary Key: []\\nSample rows from mobile_data:\\n(datetime.date(2024, 5, 22), \\'CIOT000000001\\', \\'00000000001\\', \\'00:00:00:00:00:01\\', 1, 8869, \\'物联网\\', \\'GM220-S\\', \\'济南\\', \\'章丘区\\', \\'家客铁通章丘-绣惠乡镇网格\\', \\'大高自然村\\', \\'zoneID0001\\', \\'FTTH\\', \\'分纤箱001\\', \\'SD0000001\\', \\'PON口00001\\', \\'家庭有线宽带\\', \\'分光器0001\\', \\'OLT0001\\', 14)\\n(datetime.date(2024, 5, 22), \\'CIOT000000002\\', \\'00000000002\\', \\'00:00:00:00:00:02\\', 1, 26, \\'华为\\', \\'HS8545M\\', \\'济南\\', \\'商河县\\', \\'家客铁通商河-玉皇庙乡镇网格-杨庄铺1装维组\\', \\'北乔家村-自然村\\', \\'zoneID0002\\', \\'FTTH\\', \\'分纤箱002\\', \\'SD0000002\\', \\'PON口00002\\', \\'家庭有线宽带\\', \\'分光器0002\\', \\'OLT0002\\', 14)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlalchemy\n",
    "import tool\n",
    "user = \"root\"\n",
    "password=\"root\"\n",
    "host=\"127.0.0.1\"\n",
    "database=\"text2sql\"\n",
    "engine = sqlalchemy.create_engine(f'mysql+pymysql://{user}:{password}@{host}/{database}')\n",
    "s = tool.get_table_schema(engine)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sqlite3\n",
    "from constants import DATABASE_PATH_PATTERN, DEV_JSON_PATH, TRAIN_JSON_PATH\n",
    "import util\n",
    "import json\n",
    "import sqlparse\n",
    "from thefuzz import process\n",
    "\n",
    "with open(\n",
    "    \"/home/bingxing2/home/scx8900/projects/bird_train/train/train_tables.json\", \"r\"\n",
    ") as input_file:\n",
    "    table_info = json.load(input_file)\n",
    "comment_map = {}\n",
    "for table in table_info:\n",
    "    curr = {}\n",
    "    table_names_original = table[\"table_names_original\"]\n",
    "    column_names = table[\"column_names\"]\n",
    "    column_names_original = table[\"column_names_original\"]\n",
    "    for i in range(len(column_names_original)):\n",
    "        table_index = column_names_original[i][0]\n",
    "        if table_index<0:\n",
    "            continue\n",
    "        table_name = table_names_original[table_index].lower()\n",
    "        format_column_name = f\"{table_name}.{column_names_original[i][1]}\".lower()\n",
    "        comment = column_names[i][1]\n",
    "        curr[format_column_name] = comment\n",
    "    comment_map[table[\"db_id\"]] = curr\n",
    "\n",
    "with open(\"bird_train_comment.json\", \"w\") as output_file:\n",
    "    json.dump(comment_map,output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../bird_train/train/train.json\", \"r\") as dataset_file:\n",
    "    dataset = json.load(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: no such column: t2.stuid\n",
      "Detailed error message: ('no such column: t2.stuid',)\n",
      "SQLite error code: 1\n",
      "SQLite error name: SQLITE_ERROR\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "import sqlite3\n",
    "from constants import DATABASE_PATH_PATTERN, DEV_JSON_PATH, TRAIN_JSON_PATH\n",
    "import util\n",
    "import json\n",
    "import sqlparse\n",
    "from thefuzz import process\n",
    "\n",
    "db_id = \"pets_1\"\n",
    "db_path = DATABASE_PATH_PATTERN.format(db_id=db_id)\n",
    "sqlite3.enable_callback_tracebacks(True)\n",
    "try:\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # 创建一个游标对象\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # 执行一个SQL语句\n",
    "    cursor.execute('SELECT stuid FROM student EXCEPT SELECT t2.stuid FROM has_pet AS t1 JOIN pets AS t2 ON t1.petid = t2.petid WHERE t2.pettype = \"cat\";')\n",
    "\n",
    "    # 获取查询结果\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # 打印结果\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "\n",
    "\n",
    "except sqlite3.Error as e:\n",
    "    # 打印SQLite错误信息\n",
    "    print(f\"An error occurred: {e.args[0]}\")\n",
    "    # 如果有详细信息，打印详细信息\n",
    "    # if len(e.args) > 1:\n",
    "    print(f\"Detailed error message: {e.args}\")\n",
    "    # 打印SQLite错误码和错误名称\n",
    "    print(f\"SQLite error code: {e.sqlite_errorcode}\")\n",
    "    print(f\"SQLite error name: {e.sqlite_errorname}\")\n",
    "\n",
    "finally:\n",
    "    # 关闭游标和数据库连接\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if conn:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bird_alpaca.json\", \"w\") as f:\n",
    "    json.dump(bird_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(TRAIN_JSON_PATH, \"r\") as dataset_file:\n",
    "    dataset = json.load(dataset_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = []\n",
    "for case in dataset[:]:\n",
    "    db_path = DATABASE_PATH_PATTERN.format(db_id=case[\"db_id\"])\n",
    "    schema = util.get_sqlite_alpaca_sft_prompt_schema(db_path)\n",
    "\n",
    "    schema_str = \"\"\n",
    "    for table, columns in schema.items():\n",
    "        columns_str = \", \".join(columns)\n",
    "        schema_str += f\"{table}[{columns_str}]\"\n",
    "        schema_str += \"\\n\"\n",
    "    instruction = INSTRUCTION_PATTERN.format(\n",
    "        schema=schema_str,\n",
    "        question=case[\"question\"],\n",
    "    )\n",
    "    sql = case[\"query\"]\n",
    "    sql = \" \".join(sql.split())\n",
    "    output = OUTPUT_PATTERN.format(sql=sql)\n",
    "    # print(instruction)\n",
    "    # print(output)\n",
    "    final_dataset.append({\"instruction\": instruction, \"output\": output})\n",
    "with open(\"spider_alpaca.json\", \"w\") as f:\n",
    "    json.dump(final_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8659\n",
      "9428\n",
      "17613\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"spider_alpaca.json\", \"r\") as dataset_file:\n",
    "    spider = json.load(dataset_file)\n",
    "with open(\"bird_alpaca.json\", \"r\") as dataset_file:\n",
    "    bird = json.load(dataset_file)\n",
    "print(len(spider))\n",
    "print(len(bird))\n",
    "\n",
    "all_set = []\n",
    "for case in spider:\n",
    "    if len(case[\"instruction\"]) < 10000:\n",
    "        case[\"system\"] = \"You are a database expert.\\n\"\n",
    "        all_set.append(case)\n",
    "for case in bird:\n",
    "    if len(case[\"instruction\"]) < 10000:\n",
    "        case[\"system\"] = \"You are a database expert.\\n\"\n",
    "        all_set.append(case)\n",
    "print(len(all_set))\n",
    "with open(\"text2sql.json\", \"w\") as f:\n",
    "    json.dump(all_set, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "15535\n"
     ]
    }
   ],
   "source": [
    "all_set = spider+bird_trunc\n",
    "max_len=0\n",
    "all_trunc = []\n",
    "for case in all_set:\n",
    "    if len(case[\"instruction\"]) < 4000:\n",
    "        all_trunc.append(case)\n",
    "# print(all_trunc)\n",
    "print(type(all_trunc))\n",
    "print(len(all_trunc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"spider_bird.json\", \"w\") as f:\n",
    "    json.dump(all_trunc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from constants import DATABASE_PATH_PATTERN, DEV_JSON_PATH\n",
    "import util\n",
    "\n",
    "print(torch.version.cuda)\n",
    "model_path = \"../deepseek-coder-6.7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4551015\n"
     ]
    }
   ],
   "source": [
    "from prompt import SYSTEM_PROMPT\n",
    "\n",
    "\n",
    "with open(\"cot_dataset/train/step_2.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "total = 0 \n",
    "for case in dataset:\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": case[\"instruction\"],\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": case[\"output\"]},\n",
    "    ]\n",
    "    tokens = len(tokenizer.apply_chat_template(conversation, tokenize=True))\n",
    "    total += tokens\n",
    "print(total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
